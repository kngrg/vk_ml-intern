Идея решения задачи такая: рассматриваем текст из обучающей выборки. Предобрабатываем его, то есть удаляем стоп-слова, знаки препинания и проводим лемматизацию. Для предложений, которые после лемматизации оказались пустыми, ставим специальный токен. Далее для всех сообщений с помощью модель BERT составляем вектора, которые будем использовать при обучении и предсказании разными моделями. Добавляем фичи такие как длина сообщения, процент знаков препинания в сообщении и процент спам-слов в сообщении. Эти данные передаем в модели: random forest, градиентный бустинг (XGB бустинг и CatBoost), логистическую регрессию. Для них нужно было перебрать параметры по сетке, но мне не хватило времени, поэтому выложена вторая версия (vk_intern_cut.ipynb), в которой продемонстрирован функционал на срезе 100 сообщений. Полную программу можно найти в vk_intern.ipynb. Далее выбирается лучшая модель, для нее загружается датасет с сообщениями, модель обучается на всем тренировочном датасете и предсказывает вероятности на тестовом датасете. Эти вероятности записываются в csv-файл.
